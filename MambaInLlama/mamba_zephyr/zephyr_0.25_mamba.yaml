model_name: HuggingFaceH4/zephyr-7b-beta
ssm_layers: [0, 4, 8, 12, 16, 20, 24, 28]
kl_weight: 0.1
ce_weight: 1
do_eval: false
train_datasets_path: [PSEUDO_LABEL_PATH_1, PSEUDO_LABEL_PATH_2]
output_dir: zephyr_0.25_mamba/
seed: 42
save_steps: 5000
warmup_steps: 1500
per_device_train_batch_size: 2
per_device_eval_batch_size: 4
num_train_epochs: 1
gradient_accumulation_steps: 4
lr_scheduler_type: cosine
learning_rate: 8.0e-5
max_grad_norm: 0.1