{
    "LanguageModel": {
        "input": {
            "vocab_size": 49152,
            "pad_vocab_size_multiple": 8,
            "tie_word_embeddings": true,
            "lm_head_bias": false
        }
    },
    "MixerModel": {
        "input": {
            "d_model": 2048,
            "n_layer": 24,
            "lm_head_prenorm": "layer",
            "rotary_emb": true,
            "rotary_emb_config": {
                "max_position_embeddings": 8192,
                "rope_scaling": null,
                "rope_theta": 130000,
                "rope_interleaved": false,
                "head_dim": 64,
                "hidden_size": 2048,
                "num_attention_heads": 32
            }
        }
    },
    "Block1": {
        "n_layers":32,
        "BlockType":"modules.llama_block",
        "block_input":{
            "resid_dropout":0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 8192
        },
        "CoreType":"modules.mixers.llama_attention",
        "core_input":{
            "attention_bias": false,
            "attention_dropout": 0.0,
            "bos_token_id": 0,
            "eos_token_id": 0,
            "head_dim": 64,
            "hidden_act": "silu",
            "hidden_size": 2048,
            "initializer_range": 0.02,
            "intermediate_size": 8192,
            "max_position_embeddings": 8192,
            "mlp_bias": false,
            "model_type": "llama",
            "num_attention_heads": 32,
            "num_hidden_layers": 24,
            "num_key_value_heads": 32,
            "pretraining_tp": 1,
            "rms_norm_eps": 1e-05,
            "rope_scaling": null,
            "rope_theta": 130000,
            "tie_word_embeddings": true,
            "torch_dtype": "bfloat16",
            "transformers_version": "4.48.1",
            "use_cache": true,
            "vocab_size": 49152
       }
    }
}
