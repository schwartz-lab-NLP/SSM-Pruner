{
  "LanguageModel": {
    "input": {
      "vocab_size": 49152,
      "pad_vocab_size_multiple": 8,
      "tie_word_embeddings": true,
      "lm_head_bias": false
    }
  },
  "MixerModel": {
    "input": {
      "d_model": 960,
      "n_layer": 32,
      "lm_head_prenorm": "rms",
      "rms_norm_eps": 1e-05
    }
  },
  "Block0": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block1": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block2": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block3": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block4": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block5": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block6": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block7": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block8": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block9": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block10": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block11": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block12": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block13": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block14": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block15": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block16": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block17": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block18": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block19": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block20": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block21": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block22": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block23": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block24": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block25": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block26": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block27": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block28": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block29": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  },

  "Block30": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
        "block_input": {
            "resid_dropout": 0.0,
            "rms_norm_eps": 1e-05
        },
        "mlp": {
            "intermediate_size": 2560
        },
        "CoreType": "modules.mixers.discrete_mamba2",
        "core_input": {
            "d_state": 64,
            "n_v_heads": 15,
            "n_qk_heads": 15,
            "d_conv": 3,
            "conv_bias": true,
            "expand": 1,
            "chunk_size": 128,
            "activation": "identity",
            "bias": false
        }
  },
  "Block31": {
    "n_layers": 1,
    "BlockType": "modules.llama_block",
    "block_input": {
      "resid_dropout": 0.0,
      "rms_norm_eps": 1e-05
    },
    "mlp": {
      "intermediate_size": 2560
    },
    "CoreType": "modules.mixers.llama_attention",
    "core_input": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 0,
      "eos_token_id": 0,
      "hidden_act": "silu",
      "hidden_size": 960,
      "initializer_range": 0.02,
      "intermediate_size": 2560,
      "max_position_embeddings": 2048,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 15,
      "num_hidden_layers": 32,
      "num_key_value_heads": 5,
      "pretraining_tp": 1,
      "rms_norm_eps": 1e-05,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "use_cache": true,
      "vocab_size": 49152
    }
  }
}